__import__: 
   - experiments/defaults.yaml

env_params:
  goal_mode: "walk"
  visualize_reward: true
   
rollout_params:
  render: false
  render_initial: false
  render_eval: false
  record: false
  only_final_reward: false
  use_env_states: true
  logging: true
  task_horizon: 200

controller: "mpc-icem-torch"
controller_params:
  horizon: 30
  num_simulated_trajectories: 64
  factor_decrease_num: 1
  cost_along_trajectory: "sum"
  use_env_reward: false
  action_sampler_params: 
    opt_iterations: 5
    elites_size: 10
    alpha: 0.1
    init_std: 0.5
    relative_init: true
    execute_best_elite: true
    keep_previous_elites: true
    shift_elites_over_time: true
    finetune_first_action: false
    fraction_elites_reused: 0.3
    use_mean_actions: false
    colored_noise: true
    noise_beta: 2.5
    use_ensemble_cost_std: false
  verbose: false
  do_visualize_plan: false
  use_async_action: false
  logging: true

initial_controller: "none"
initial_controller_params: {}
initial_number_of_rollouts: 0

forward_model_params:
    train_params:
        epochs: 0
        iterations: 0

number_of_rollouts: 100
training_iterations: 1

append_data: true
append_data_eval: false

checkpoints:
  load: false
  save: true
  save_every_n_iter: 2
  restart_every_n_iter: null

device: "cuda:0"

working_dir: "experiments/results/roboyoga_quadruped/zero_shot/walk"
